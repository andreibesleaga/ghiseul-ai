
# Ports Overview:
# 8000 - vllm (external)
# 9000 - textgen
# 7000 - tei (embedding)
# 6000 - embedding wrapper
# 7100 - tei_reranker
# 6100 - reranker wrapper

services:
  dataprep-arango-service:
    image: ${REGISTRY:-opea}/dataprep:${TAG:-latest}
    container_name: dataprep-arango-server
    depends_on:
      # - arango-vector-db
      - embedding
      - vllm
    ports:
      - "6007:5000"
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "2"
    #       memory: "4G"
    #     reservations:
    #       cpus: "1"
    #       memory: "2G"
    environment:
      DATAPREP_COMPONENT_NAME: "OPEA_DATAPREP_ARANGODB"
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      VLLM_API_KEY: ${VLLM_API_KEY}
      VLLM_ENDPOINT: http://e2e-109-198:8000
      VLLM_MODEL_ID: Qwen/Qwen3-1.7B # ${LLM_MODEL_ID}
      TEI_EMBEDDING_ENDPOINT: http://e2e-109-198:6000 #  http://tei-embedding-service:80
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      ARANGO_URL: http://e2e-109-51:8529
      ARANGO_USERNAME: ${ARANGO_USERNAME}
      ARANGO_PASSWORD: ${ARANGO_PASSWORD}
      ARANGO_DB_NAME: arangogogo # ${ARANGO_DB_NAME}
      ARANGO_INSERT_ASYNC: ${DATAPREP_ARANGO_INSERT_ASYNC}
      ARANGO_GRAPH_NAME: ${DATAPREP_ARANGO_GRAPH_NAME}
      ARANGO_USE_GRAPH_NAME: ${DATAPREP_ARANGO_USE_GRAPH_NAME}
      CHUNK_SIZE: ${DATAPREP_CHUNK_SIZE}
      CHUNK_OVERLAP: ${DATAPREP_CHUNK_OVERLAP}
      EMBED_SOURCE_DOCUMENTS: ${DATAPREP_EMBED_SOURCE_DOCUMENTS}
      EMBED_NODES: ${DATAPREP_EMBED_NODES}
      EMBED_RELATIONSHIPS: ${DATAPREP_EMBED_RELATIONSHIPS}
      NODE_PROPERTIES: ${DATAPREP_NODE_PROPERTIES}
      RELATIONSHIP_PROPERTIES: ${DATAPREP_RELATIONSHIP_PROPERTIES}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_CHAT_ENABLED: ${DATAPREP_OPENAI_CHAT_ENABLED}
      OPENAI_EMBED_ENABLED: ${DATAPREP_OPENAI_EMBED_ENABLED}
      LOGFLAG: ${LOGFLAG}
    ipc: host
    restart: "no"

  retriever-arango-service:
    image: ${REGISTRY:-opea}/retriever:${TAG:-latest}
    container_name: retriever-arango-server
    depends_on:
      # - arango-vector-db
      - embedding
    ports:
      - "7025:7025"
    ipc: host
    # deploy:
    #   resources:
    #     limits:
    #       cpus: "2"
    #       memory: "4G"
    #     reservations:
    #       cpus: "1"
    #       memory: "2G"
    environment:
      RETRIEVER_COMPONENT_NAME: "OPEA_RETRIEVER_ARANGODB"
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      VLLM_API_KEY: ${VLLM_API_KEY}
      VLLM_ENDPOINT: http://e2e-109-198:8000
      VLLM_MODEL_ID: Qwen/Qwen3-1.7B # ${LLM_MODEL_ID}
      ARANGO_URL: http://e2e-109-51:8529
      ARANGO_USERNAME: ${ARANGO_USERNAME}
      ARANGO_PASSWORD: ${ARANGO_PASSWORD}
      ARANGO_DB_NAME: ${ARANGO_DB_NAME}
      ARANGO_TRAVERSAL_ENABLED: ${RETRIEVER_ARANGO_TRAVERSAL_ENABLED}
      ARANGO_TRAVERSAL_MAX_DEPTH: ${RETRIEVER_ARANGO_TRAVERSAL_MAX_DEPTH}
      ARANGO_USE_APPROX_SEARCH: ${RETRIEVER_ARANGO_USE_APPROX_SEARCH}
      TEI_EMBEDDING_ENDPOINT: http://e2e-109-198:6000 # http://tei-embedding-service:80
      TEI_EMBED_MODEL: ${TEI_EMBED_MODEL}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_EMBED_MODEL: ${RETRIEVER_OPENAI_EMBED_MODEL}
      OPENAI_EMBED_ENABLED: ${RETRIEVER_OPENAI_EMBED_ENABLED}
      OPENAI_CHAT_ENABLED: ${RETRIEVER_OPENAI_CHAT_ENABLED}
      SUMMARIZER_ENABLED: ${RETRIEVER_SUMMARIZER_ENABLED}
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      LOGFLAG: ${LOGFLAG}
    restart: "no"


  chatqna-xeon-backend-server:
    image: ${REGISTRY:-opea}/chatqna:${TAG:-latest}
    container_name: chatqna-xeon-backend-server
    depends_on:
      # - arango-vector-db
      - embedding
      - retriever-arango-service
      - reranker
      - vllm
    ports:
      - "8888:8888"
    environment:
      - no_proxy=${no_proxy}
      - https_proxy=${https_proxy}
      - http_proxy=${http_proxy}
      - MEGA_SERVICE_HOST_IP=chatqna-xeon-backend-server
      - EMBEDDING_SERVER_HOST_IP=e2e-109-198
      - EMBEDDING_SERVER_PORT=6000
      - RETRIEVER_SERVICE_HOST_IP=retriever-arango-service
      - RERANK_SERVER_HOST_IP=e2e-109-198
      - RERANK_SERVER_PORT=6100
      - LLM_SERVER_HOST_IP=e2e-109-198
      - LLM_SERVER_PORT=9000 # TODO: vLLM (8000) or TextGen (9000)?
      - LLM_MODEL=Qwen/Qwen3-0.6B # ibm-granite/granite-3.2-2b-instruct # ${LLM_MODEL_ID}
      - LOGFLAG=${LOGFLAG}
    ipc: host
    restart: always

  # NOTE: Replaced with ChatUI built by ITU
  # chatqna-xeon-ui-server:
  #   image: ${REGISTRY:-opea}/chatqna-ui:${TAG:-latest}
  #   container_name: chatqna-xeon-ui-server
  #   depends_on:
  #     - chatqna-xeon-backend-server
  #   ports:
  #     - "5173:5173"
  #   environment:
  #     - no_proxy=${no_proxy}
  #     - https_proxy=${https_proxy}
  #     - http_proxy=${http_proxy}
  #     - UPLOAD_FILE_BASE_URL=http://localhost:6007/v1/dataprep/ingest
  #   ipc: host
  #   restart: always

  chatqna-xeon-nginx-server:
    image: ${REGISTRY:-opea}/nginx:${TAG:-latest}
    container_name: chatqna-xeon-nginx-server
    depends_on:
      - chatqna-xeon-backend-server
      - chatqna-xeon-ui-server
    ports:
      - "${NGINX_PORT:-80}:80"
    environment:
      - no_proxy=${no_proxy}
      - https_proxy=${https_proxy}
      - http_proxy=${http_proxy}
      - FRONTEND_SERVICE_IP=chatqna-xeon-ui-server
      - FRONTEND_SERVICE_PORT=5173
      - BACKEND_SERVICE_NAME=chatqna
      - BACKEND_SERVICE_IP=chatqna-xeon-backend-server
      - BACKEND_SERVICE_PORT=8888
      - DATAPREP_SERVICE_IP=dataprep-arango-service
      - DATAPREP_SERVICE_PORT=5000
    ipc: host
    restart: always

  vllm:
    container_name: vllm-vllm-2
    image: vllm/vllm-openai:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8000:8000"
    command: >
      --model Qwen/Qwen3-1.7B
      --gpu_memory_utilization 0.5
      --served-model-name Qwen3-1.7B
      --max_model_len 8192
      --dtype=half
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - /root/.cache/huggingface:/root/.cache/huggingface
    networks:
      - llm_network
    ipc: "host"
    restart: no

  textgen:
    container_name: llm-textgen
    build:
      context: ./GenAIComps
      dockerfile: comps/llms/src/text-generation/Dockerfile
    environment:
      - LLM_ENDPOINT=http://vllm:8000
      - LLM_MODEL_ID=Qwen3-1.7B
    ports:
      - "9000:9000"
    ipc: "host"
    networks:
      - llm_network
    restart: unless-stopped

  vllm-translation-guardrail:
    container_name: vllm-vllm-translation-guardrail
    image: vllm/vllm-openai:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "9031:9031"
    command: >
      --model google/gemma-3-1b-it
      --gpu_memory_utilization 0.5
      --served-model-name gemma-3-1b-it
      --max_model_len 1024
      --dtype=half
      --port 9031
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - /root/.cache/huggingface:/root/.cache/huggingface
    networks:
      - llm_network
    ipc: "host"
    restart: no

  guardrail:
    container_name: llm-guardrail
    build:
      context: ./GenAIComps
      dockerfile: comps/guardrails/src/guardrails/Dockerfile
    environment:
      - LLM_ENDPOINT=http://vllm-translation-guardrail:9031 ###
      - LLM_MODEL_ID=gemma-3-1b-it
      - SAFETY_GUARD_MODEL_ID=gemma-3-1b-it
      - GUARDRAILS_COMPONENT_NAME=OPEA_GEMMA_GUARD
    ports:
      - "9090:9090"
    ipc: "host"
    networks:
      - llm_network
    restart: no

  translation:
    container_name: llm-translation
    build:
      context: ./GenAIComps
      dockerfile: comps/llms/src/translation-generation/Dockerfile
    environment:
      - LLM_ENDPOINT=http://vllm-translation-guardrail:9031 ###
      - LLM_MODEL_ID=gemma-3-1b-it
    ports:
      - "9030:9030"
    ipc: "host"
    networks:
      - llm_network
    restart: no

  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.7
    container_name: tei-embedding-serving
    entrypoint: /bin/sh -c "text-embeddings-router --json-output --model-id ${EMBEDDING_MODEL_ID}"
    runtime: nvidia
    ports:
      - "7000:80"
    volumes:
      - "${DATA_PATH:-./data}:/data"
      # Add exact path to libcuda.so.1 - needs to be located and mapped
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
      - /usr/lib/x86_64-linux-gnu/libcudart.so.11.0:/usr/lib/x86_64-linux-gnu/libcudart.so.11.0
    shm_size: 2g
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - RUST_BACKTRACE=1
      - NVIDIA_VISIBLE_DEVICES=all
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 10s
      timeout: 6s
      retries: 48
    networks:
      - llm_network
    restart: unless-stopped

  embedding:
    container_name: embedding
    build:
      context: ./GenAIComps
      dockerfile: comps/embeddings/src/Dockerfile
    environment:
      - EMBEDDING_MODEL_ENDPOINT=http://tei:80
      - TEI_EMBEDDING_ENDPOINT=http://tei:80
      - EMBEDDING_MODEL_ID=bge-base-en-v1.5
    ports:
      - "6000:6000"
    ipc: "host"
    networks:
      - llm_network
    restart: unless-stopped

  #Adding reranker microservice
  tei_reranker:
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.7
    container_name: tei-reranker-serving
    entrypoint: /bin/sh -c "text-embeddings-router --json-output --model-id ${RERANKER_MODEL_ID}"
    runtime: nvidia
    ports:
      - "7100:80"
    volumes:
      - "${DATA_PATH:-./data}:/data"
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
      - /usr/lib/x86_64-linux-gnu/libcudart.so.11.0:/usr/lib/x86_64-linux-gnu/libcudart.so.11.0
    shm_size: 2g
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - RUST_BACKTRACE=1
      - NVIDIA_VISIBLE_DEVICES=all
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 10s
      timeout: 6s
      retries: 48
    networks:
      - llm_network
    restart: unless-stopped

  reranker:
    container_name: reranker
    build:
      context: ./GenAIComps
      dockerfile: comps/rerankings/src/Dockerfile
    environment:
      - RERANKER_MODEL_ENDPOINT=http://tei_reranker:80
      - TEI_RERANKING_ENDPOINT=http://tei_reranker:80
      - RERANKER_MODEL_ID=bge-reranker-base
    ports:
      - "6100:8000"
    ipc: "host"
    networks:
      - llm_network
    restart: unless-stopped

networks:
  llm_network:
    driver: bridge
