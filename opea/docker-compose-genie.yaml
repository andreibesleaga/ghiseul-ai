
# Ports Overview:
# 80 - default OPEA UI nginx server (chatqna-xeon-nginx-server)
# 5173 - default OPEA UI frontend (chatqna-xeon-ui-server)
# 6000 - embedding wrapper
# 6007 - dataprep API endpoint
# 6100 - reranker wrapper
# 7000 - TEI (embedding inference server)
# 7025 - retriever-arango-dev
# 7100 - TEI reranker (inference server)
# 8000 - vLLM (main LLM server)
# 8888 - chatqna backend server
# 9000 - textgen wrapper
# 9030 - translation wrapper
# 9031 - vllm-translation-guardrail (guarded LLM)
# 9090 - guardrail (guard logic endpoint)
# 6666 - http-service (for auth token retrieval)

name: opea
services:

  http-service:
    build:
      context: ./GenAIComps
      dockerfile: comps/third_parties/httpservices/Dockerfile
    image: http-service:latest
    container_name: http-service
    environment:
      AUTH_SERVICE_URL: "https://genie-ai.itu.int/"
      AUTH_SERVICE_USERNAME: "genie-ai-manager"
      AUTH_SERVICE_PASSWORD: "1357924680+Manager"
    ports:
      - "6666:6666"
    networks:
      - llm_network
    restart: "no"

  dataprep-arango-service:
    build:
      context: ./GenAIComps 
      dockerfile: comps/dataprep/src/Dockerfile
    image: dataprep-dev:latest # image: ${REGISTRY:-opea}/dataprep:${TAG:-latest}
    container_name: dataprep-arango-dev
    depends_on:
      - embedding
      - vllm
      - guardrail
    volumes:
      - ./GenAIComps/comps:/home/user/comps
    ports:
      - "6007:5000"
    environment:
      DATAPREP_COMPONENT_NAME: "OPEA_DATAPREP_ARANGODB"
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      VLLM_ENDPOINT: http://vllm:8000
      VLLM_MODEL_ID: ibm-granite/granite-3.3-2b-instruct # ${LLM_MODEL_ID}
      TEI_EMBEDDING_ENDPOINT: http://embedding:6000
      TEI_EMBED_MODEL: ${TEI_EMBED_MODEL}
      GUARDRAIL_URL: http://guardrail:9090/v1/guardrails
      GUARDRAIL_ENABLED: false
      DOC_REPO_URL: http://localhost:3001 # Not used for current version. Might be helpful if directly calling Document Repository APIs
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      E2E_CPU_URL: http://e2e-109-51:3000
      ARANGO_URL: http://e2e-109-51:8529
      ARANGO_USERNAME: ${ARANGO_USERNAME}
      ARANGO_PASSWORD: ${ARANGO_PASSWORD}
      ARANGO_DB_NAME: ${ARANGO_DB_NAME}
      ARANGO_GRAPH_NAME: ${DATAPREP_ARANGO_GRAPH_NAME}
      ARANGO_INSERT_ASYNC: ${DATAPREP_ARANGO_INSERT_ASYNC}
      ARANGO_USE_GRAPH_NAME: ${DATAPREP_ARANGO_USE_GRAPH_NAME}
      CHUNK_SIZE: ${DATAPREP_CHUNK_SIZE}
      CHUNK_OVERLAP: ${DATAPREP_CHUNK_OVERLAP}
      EMBED_SOURCE_DOCUMENTS: ${DATAPREP_EMBED_SOURCE_DOCUMENTS}
      EMBED_NODES: ${DATAPREP_EMBED_NODES}
      EMBED_RELATIONSHIPS: ${DATAPREP_EMBED_RELATIONSHIPS}
      NODE_PROPERTIES: ${DATAPREP_NODE_PROPERTIES}
      RELATIONSHIP_PROPERTIES: ${DATAPREP_RELATIONSHIP_PROPERTIES}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_CHAT_ENABLED: ${DATAPREP_OPENAI_CHAT_ENABLED}
      OPENAI_EMBED_ENABLED: ${DATAPREP_OPENAI_EMBED_ENABLED}
      LOGFLAG: ${LOGFLAG}
      GET_AUTH_TOKEN_URL: http://http-service:6666/get-token
    networks:
      - llm_network
    ipc: "host"
    restart: "no"

  retriever-arango-service:
    build:
      context: ./GenAIComps 
      dockerfile: comps/retrievers/src/Dockerfile
    image: retriever-dev:latest # image: ${REGISTRY:-opea}/retriever:${TAG:-latest}
    container_name: retriever-arango-dev
    depends_on:
      - embedding
    ports:
      - "7025:7025"
    ipc: host
    volumes:
      - ./GenAIComps/comps:/home/user/comps
    environment:
      RETRIEVER_COMPONENT_NAME: "OPEA_RETRIEVER_ARANGODB"
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      VLLM_API_KEY: ${VLLM_API_KEY}
      VLLM_ENDPOINT: http://vllm:8000 # Only used for Result Summarization
      VLLM_MODEL_ID: ibm-granite/granite-3.3-2b-instruct # ${LLM_MODEL_ID}
      ARANGO_URL: http://e2e-109-51:8529
      ARANGO_USERNAME: ${ARANGO_USERNAME}
      ARANGO_PASSWORD: ${ARANGO_PASSWORD}
      ARANGO_DB_NAME: ${ARANGO_DB_NAME}
      ARANGO_GRAPH_NAME: ${RETRIEVER_ARANGO_GRAPH_NAME}
      ARANGO_SEARCH_START: ${RETRIEVER_ARANGO_SEARCH_START}
      ARANGO_SEARCH_MODE: ${RETRIEVER_ARANGO_SEARCH_MODE}
      ARANGO_TRAVERSAL_ENABLED: ${RETRIEVER_ARANGO_TRAVERSAL_ENABLED}
      ARANGO_TRAVERSAL_MAX_DEPTH: ${RETRIEVER_ARANGO_TRAVERSAL_MAX_DEPTH}
      ARANGO_USE_APPROX_SEARCH: ${RETRIEVER_ARANGO_USE_APPROX_SEARCH}
      TEI_EMBEDDING_ENDPOINT: http://embedding:6000
      TEI_EMBED_MODEL: ${TEI_EMBED_MODEL}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_EMBED_MODEL: ${RETRIEVER_OPENAI_EMBED_MODEL}
      OPENAI_EMBED_ENABLED: ${RETRIEVER_OPENAI_EMBED_ENABLED}
      OPENAI_CHAT_ENABLED: ${RETRIEVER_OPENAI_CHAT_ENABLED}
      SUMMARIZER_ENABLED: ${RETRIEVER_SUMMARIZER_ENABLED}
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      LOGFLAG: ${LOGFLAG}
    networks:
      - llm_network
    restart: "no"

  chatqna-xeon-backend-server:
    build:
      context: . 
      dockerfile: ./ChatQnA/Dockerfile_genieai
    image: chatqna-genieai:latest # image: ${REGISTRY:-opea}/chatqna:${TAG:-latest}
    container_name: chatqna-xeon-backend-server
    volumes:
      - .:/app
    working_dir: /app/ChatQnA
    depends_on:
      - embedding
      - retriever-arango-service
      - reranker
      - vllm
    ports:
      - "8888:8888"
    environment:
      - no_proxy=${no_proxy}
      - https_proxy=${https_proxy}
      - http_proxy=${http_proxy}
      - CHATQNA_TYPE=CHATQNA_MACDAVID
      - MEGA_SERVICE_HOST_IP=chatqna-xeon-backend-server
      - EMBEDDING_SERVER_HOST_IP=tei
      - EMBEDDING_SERVER_PORT=80
      - RETRIEVER_SERVICE_HOST_IP=retriever-arango-service
      - RERANK_SERVER_HOST_IP=tei_reranker
      - RERANK_SERVER_PORT=80
      - LLM_SERVER_HOST_IP=vllm
      - LLM_SERVER_PORT=8000
      - LLM_MODEL=ibm-granite/granite-3.3-2b-instruct # ${LLM_MODEL_ID}
      - LLM_TRANS_MODEL=google/gemma-3-1b-it
      - GUARDRAIL_SERVICE_HOST_IP=guardrail
      - GUARDRAIL_SERVICE_PORT=9090
      - TRANSLATION_SERVICE_HOST_IP=vllm-translation-guardrail
      - TRANSLATION_SERVICE_PORT=9031
      - LOGFLAG=${LOGFLAG}
      - RETRIEVER_ARANGO_SEARCH_START=${RETRIEVER_ARANGO_SEARCH_START}
      - DOC_REPO_URL=http://e2e-109-51:3001
      - GET_AUTH_TOKEN_URL=http://http-service:6666/get-token
    networks:
      - llm_network
    ipc: "host"
    restart: "no"

  chatqna-xeon-ui-server:
    image: ${REGISTRY:-opea}/chatqna-ui:${TAG:-latest}
    container_name: chatqna-xeon-ui-server
    depends_on:
      - chatqna-xeon-backend-server
    ports:
      - "5173:5173"
    environment:
      - no_proxy=${no_proxy}
      - https_proxy=${https_proxy}
      - http_proxy=${http_proxy}
      - UPLOAD_FILE_BASE_URL=http://localhost:6007/v1/dataprep/ingest
    ipc: "host"
    networks:
      - llm_network
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5173/"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  chatqna-xeon-nginx-server:
    image: ${REGISTRY:-opea}/nginx:${TAG:-latest}
    container_name: chatqna-xeon-nginx-server
    depends_on:
      chatqna-xeon-backend-server:
        condition: service_started
      chatqna-xeon-ui-server:
        condition: service_healthy
    ports:
      - "${NGINX_PORT:-80}:80"
    environment:
      - no_proxy=${no_proxy}
      - https_proxy=${https_proxy}
      - http_proxy=${http_proxy}
      - FRONTEND_SERVICE_IP=chatqna-xeon-ui-server
      - FRONTEND_SERVICE_PORT=5173
      - BACKEND_SERVICE_NAME=chatqna
      - BACKEND_SERVICE_IP=chatqna-xeon-backend-server
      - BACKEND_SERVICE_PORT=8888
      - DATAPREP_SERVICE_IP=dataprep-arango-service
      - DATAPREP_SERVICE_PORT=5000
    ipc: "host"
    networks:
      - llm_network
    restart: "no"

  vllm:
    container_name: vllm-vllm-2
    image: vllm/vllm-openai:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "8000:8000"
    command: >
      --model ibm-granite/granite-3.3-2b-instruct
      --gpu_memory_utilization 0.6
      --served-model-name ibm-granite/granite-3.3-2b-instruct
      --max_model_len 4096
      --dtype=half
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - /root/.cache/huggingface:/root/.cache/huggingface
    networks:
      - llm_network
    ipc: "host"
    restart: "no"

  textgen:
    container_name: llm-textgen
    build:
      context: ./GenAIComps
      dockerfile: comps/llms/src/text-generation/Dockerfile
    environment:
      - LLM_ENDPOINT=http://vllm:8000
      - LLM_MODEL_ID=ibm-granite/granite-3.3-2b-instruct
    ports:
      - "9000:9000"
    ipc: "host"
    networks:
      - llm_network
    restart: unless-stopped

  vllm-translation-guardrail:
    container_name: vllm-vllm-translation-guardrail
    image: vllm/vllm-openai:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "9031:9031"
    command: >
      --model google/gemma-3-1b-it
      --gpu_memory_utilization 0.4
      --served-model-name gemma-3-1b-it
      --max_model_len 2048
      --dtype=half
      --port 9031
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - /root/.cache/huggingface:/root/.cache/huggingface
    networks:
      - llm_network
    ipc: "host"
    restart: "no"

  guardrail:
    container_name: llm-guardrail
    build:
      context: ./GenAIComps
      dockerfile: comps/guardrails/src/guardrails/Dockerfile
    volumes:
      - ./GenAIComps/comps:/home/user/comps
    environment:
      - LLM_ENDPOINT=http://vllm-translation-guardrail:9031
      - LLM_MODEL_ID=gemma-3-1b-it
      - SAFETY_GUARD_MODEL_ID=gemma-3-1b-it
      - GUARDRAILS_COMPONENT_NAME=OPEA_GEMMA_GUARD
    ports:
      - "9090:9090"
    ipc: "host"
    networks:
      - llm_network
    restart: "no"

  translation:
    container_name: llm-translation
    build:
      context: ./GenAIComps
      dockerfile: comps/llms/src/translation-generation/Dockerfile
    environment:
      - LLM_ENDPOINT=http://vllm-translation-guardrail:9031
      - LLM_MODEL_ID=gemma-3-1b-it
    ports:
      - "9030:9030"
    ipc: "host"
    networks:
      - llm_network
    restart: "no"

  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.7
    container_name: tei-embedding-serving
    entrypoint: /bin/sh -c "text-embeddings-router --json-output --model-id ${EMBEDDING_MODEL_ID}"
    runtime: nvidia
    ports:
      - "7000:80"
    volumes:
      - "${DATA_PATH:-./data}:/data"
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
      - /usr/lib/x86_64-linux-gnu/libcudart.so.11.0:/usr/lib/x86_64-linux-gnu/libcudart.so.11.0
    shm_size: 2g
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - RUST_BACKTRACE=1
      - NVIDIA_VISIBLE_DEVICES=all
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 10s
      timeout: 6s
      retries: 48
    networks:
      - llm_network
    restart: "no"

  embedding:
    container_name: embedding
    build:
      context: ./GenAIComps
      dockerfile: comps/embeddings/src/Dockerfile
    volumes:
      - ./GenAIComps/comps:/home/user/comps
    environment:
      - EMBEDDING_MODEL_ENDPOINT=http://tei:80
      - TEI_EMBEDDING_ENDPOINT=http://tei:80
      - EMBEDDING_MODEL_ID=bge-base-en-v1.5
    ports:
      - "6000:6000"
    ipc: "host"
    networks:
      - llm_network
    restart: "no"

  tei_reranker:
    image: ghcr.io/huggingface/text-embeddings-inference:turing-1.7
    container_name: tei-reranker-serving
    entrypoint: /bin/sh -c "text-embeddings-router --json-output --model-id ${RERANKER_MODEL_ID}"
    runtime: nvidia
    ports:
      - "7100:80"
    volumes:
      - "${DATA_PATH:-./data}:/data"
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1
      - /usr/lib/x86_64-linux-gnu/libcudart.so.11.0:/usr/lib/x86_64-linux-gnu/libcudart.so.11.0
    shm_size: 2g
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - RUST_BACKTRACE=1
      - NVIDIA_VISIBLE_DEVICES=all
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 10s
      timeout: 6s
      retries: 48
    networks:
      - llm_network
    restart: "no"

  reranker:
    container_name: reranker
    build:
      context: ./GenAIComps
      dockerfile: comps/rerankings/src/Dockerfile
    volumes:
      - ./GenAIComps/comps:/home/user/comps
    environment:
      - RERANKER_MODEL_ENDPOINT=http://tei_reranker:80
      - TEI_RERANKING_ENDPOINT=http://tei_reranker:80
      - RERANKER_MODEL_ID=bge-reranker-base
      - LOGFLAG=${LOGFLAG}
    ports:
      - "6100:8000"
    ipc: "host"
    networks:
      - llm_network
    restart: "no"

networks:
  llm_network:
    driver: bridge
