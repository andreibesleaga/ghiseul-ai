{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTNiSICWS0T9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import csv\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import markdown\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Service data collected from official websites and converted to markdown file\n",
        "\n",
        "file_path = '' #content/Kenya_Services_Info_0403.md"
      ],
      "metadata": {
        "id": "dPAtqbUuTQpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your own code for vLLM / TGI / of inference endpoint\n",
        "\n",
        "api_key = \"Denvr endpoint API\"\n",
        "endpoint = \"LLM endpoint link\""
      ],
      "metadata": {
        "id": "bu31BP6jTTsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}"
      ],
      "metadata": {
        "id": "H7Y5xMXuTVfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt_dict = \"\"\"\n",
        "Your task is to identify the entities and relations from a given text, extract them into a structured JSON format for later building knowledge graphs.\n",
        "The output should be a JSON with a single key \"RESPONSE\" containing a list of dictionaries, each with three keys - \"head\", \"tail\", and \"relation\" - with corresponding values.\n",
        "\n",
        "Here is an input example:\n",
        "\"\"Adam is a software engineer in Microsoft since 2009, and last year he got an award as the Best Talent. Additionally ...\"\"\n",
        "\n",
        "Here is a corresponding output example:\n",
        "{\n",
        "  \"RESPONSE\":\n",
        "  [\n",
        "  {\n",
        "    \"head\": \"Adam\",\n",
        "    \"relation\": \"works for\",\n",
        "    \"tail\": \"Microsoft\"\n",
        "  },\n",
        "  {\n",
        "    \"head\": \"Adam\",\n",
        "    \"relation\": \"received\",\n",
        "    \"tail\": \"Best Talent Award\"\n",
        "  },\n",
        "  ...\n",
        "  ]\n",
        "}\n",
        "\n",
        "Your output should always follow the above format. Do not include any other text in the output.\n",
        "\n",
        "Make sure that you:\n",
        "1. Extract as many entities and relations as you can, as long as the combination is unique;\n",
        "2. Maintain Entity Consistency: When extracting entities, it's vital to ensure consistency. If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"), always use the most complete identifier for that entity.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "l_TgRipGTXOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_by_header(file_path, max_chunk_size=1500):\n",
        "\n",
        "  with open(file_path, 'r') as f:\n",
        "    md_content = f.read()\n",
        "\n",
        "  html_content = markdown.markdown(md_content)\n",
        "\n",
        "  chunks = []\n",
        "  current_chunk = ''\n",
        "  for line in html_content.split('\\n'):\n",
        "    if line.startswith('<h'):  # Check for header lines\n",
        "      if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "      current_chunk = line + '\\n'  # Start a new chunk with the header\n",
        "    else:\n",
        "      current_chunk += line + '\\n'\n",
        "\n",
        "      # Check if current chunk exceeds max size\n",
        "      if len(current_chunk) >= max_chunk_size:\n",
        "        chunks.append(current_chunk)\n",
        "        current_chunk = ''  # Start a new chunk\n",
        "\n",
        "  if current_chunk:\n",
        "    chunks.append(current_chunk)  # Add the last chunk\n",
        "\n",
        "  print(f'\\nGenerated {len(chunks)} text splits')\n",
        "  average_length = len(html_content) / len(chunks)\n",
        "  print(f'Average chunk length: {average_length} characters\\n')\n",
        "\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "Cnb25cihTZf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entity_and_relation_extraction(chunk_text):\n",
        "    data = {\n",
        "    \"model\": \"meta-llama/Llama-3.3-70B-Instruct\",  # or the model name provided by the endpoint\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\",\"content\": system_prompt_dict},\n",
        "        {\"role\": \"user\", \"content\": chunk_text}\n",
        "    ],\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 2048\n",
        "    }\n",
        "\n",
        "    request_response = requests.post(endpoint, headers=headers, json=data)\n",
        "\n",
        "    try:\n",
        "      response = request_response.json()['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "      print('Useless codes: Could not process llm output\\n')\n",
        "      return 'Error'\n",
        "\n",
        "    try:\n",
        "      triplets = json.loads(response)\n",
        "      print(f\"Identified {len(triplets['RESPONSE'])} triplet(s)\")\n",
        "      return triplets       # \"triplets\" here is the formatted json output\n",
        "\n",
        "    except Exception as e:\n",
        "      print('Pre-processing llm output to convert to json')\n",
        "      for c in str(response):\n",
        "        if c != '{':\n",
        "          response = response[1:]\n",
        "        else:\n",
        "          response = response[::-1]\n",
        "          for c in str(response):\n",
        "            if c != '}':\n",
        "              response = response[1:]\n",
        "            else:\n",
        "              response = response[::-1]\n",
        "              try:\n",
        "                triplets = json.loads(response)\n",
        "                print(f\"Identified {len(triplets['RESPONSE'])} triplet(s)\")\n",
        "                return triplets\n",
        "              except:\n",
        "                print('Could not process llm output\\n')\n",
        "                return 'Error'"
      ],
      "metadata": {
        "id": "JclTlhYMTdhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entity_and_relation_extraction_pro(chunk_text):\n",
        "  returned = 'Error'\n",
        "  while returned == 'Error':\n",
        "    returned = entity_and_relation_extraction(chunk_text)\n",
        "\n",
        "  return returned"
      ],
      "metadata": {
        "id": "Q0JbYEdZTfqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_to_csv(file_path, data):\n",
        "\n",
        "    # Check if the file exists and has content\n",
        "    file_exists = os.path.exists(file_path) and os.path.getsize(file_path) > 0\n",
        "\n",
        "    with open(file_path, 'a', newline='') as csvfile:  # Use 'a' to append\n",
        "        writer = csv.writer(csvfile)\n",
        "\n",
        "        # Write header only if the file is empty or doesn't exist\n",
        "        if not file_exists:\n",
        "            writer.writerow(['head', 'relation', 'tail'])\n",
        "\n",
        "        # Write the data rows\n",
        "        try:\n",
        "          for d in data:\n",
        "            writer.writerow(d.values())\n",
        "        except Exception as e:\n",
        "          print(f'Could not write to csv: {e}')\n",
        "\n",
        "    print('âœ… Data written to CSV successfully!\\n')"
      ],
      "metadata": {
        "id": "IIItSBleThlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_splits = chunk_by_header(file_path)\n",
        "\n",
        "count = 0\n",
        "for c in content_splits:\n",
        "  count += 1\n",
        "  print(f'Processing chunk {count} of length {len(c)}...')\n",
        "  extracted_data = entity_and_relation_extraction_pro(c)\n",
        "  try:\n",
        "    write_to_csv('hard_core_kg.csv', extracted_data['RESPONSE'])\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(f'Displaying the llm output: {extracted_data} \\n')\n",
        "  print()"
      ],
      "metadata": {
        "id": "PFqBERKmTjVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}